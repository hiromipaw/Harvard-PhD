\begin{savequote}[75mm] 
A Jedi uses the Force for knowledge and defense, never for attack.
\qauthor{Yoda - The Empire Strikes Back} 
\end{savequote}

\chapter{Conclusions and discussion}

\newthought{This dissertation examined a class of privacy issues for online communication}, proposing a model for the user identity and a possible new approach to information privacy management. This work focused on the analysis of privacy violation that can be found in different scenarios, on web and mobile applications and services.
The goal was to convince the reader that, as the web is shifting towards hypermedia data models and protocols, also privacy analysis and protection have to adopt the same mindset.

The motivation behind this work was understanding how data, created by users, flows between applications and services. A very powerful example in this field is the use of federated log in mechanisms. To register to a new social application, users grant them a certain level of access to their identity data, through, for example, their Facebook, Twitter or Google accounts. This data includes details about their identity, their whereabouts and in some situations even the company they work for. Third parties, like Facebook or Google, offer log in technologies, allowing the application to identify the user and receive precise information about them. Once the user grant access to their data, the application stores it and assumes control over how it is further shared. The user will never be notified again on who is accessing their data, nor if these are transferred to third parties. We showed how this mechanism can be modified to mitigate or avoid this.

We believe that an important aspect of privacy protection is giving web users the possibility to control their digital footprints. More specifically, we are aware that privacy issues involve a plurality of complexities. This is especially true nowadays that privacy has acquired a completely different meaning because people conduct part of their existence through and on communication platforms. Privacy rights need to consider the implication of \emph{information privacy}, given that a person shares parts of their activities, interests and even thoughts with online service providers. As a consequence, the philosophical definition of privacy has evolved, while laws protecting individual privacy rights have tried to follow.

Up to now, in an online context, the right to privacy has commonly been interpreted as a right to \emph{information self-determination}. Acts typically claimed to breach online privacy concern the collection of personal information without consent, the selling of personal information and the further processing of that information. This definition of privacy breach can be considered valid until the user has direct control of the data they have created.

This work started by analysing information filtering systems. These systems have been developed to predict users' preferences, and eventually use the resulting predictions for different services, depend on users revealing their personal preferences by annotating items that are relevant to them.  At the same time, by revealing their preferences online users are exposed to possible privacy attacks and all sorts of profiling activities by legitimate and less legitimate entities.

We showed how query forgery arises, among different possible PETs, as a simple strategy in terms of infrastructure requirements, as no third parties or external entities need to be trusted by the user in order to be implemented. However, query forgery poses a trade-off between privacy and utility. Measuring utility by computing the list of useful results that a user would receive from a recommendation system, we have evaluated how three possible tag forgery techniques would perform in a social tag application. With this in mind a dataset for a real world application, rich in collaborative tagging information has been considered.

It was calculated how the performance of a recommendation system would be affected if all the users implemented a tag forgery strategy. We hence considered an adversary model where a passive privacy attacker is trying to profile a certain user. The user in response, adopts a privacy strategy aiming at concealing their actual preferences, minimising the divergence with the average population profile. The results presented show a compelling outcome regarding how implementing different PETs can affect both user privacy risk, as well as the overall recommendation utility. It was showed how in a simple experimental evaluation, of a real world application scenario, the performances degradation of a recommendation system, is small if compared to the privacy risk reduction offered by the application of these techniques.

Furthermore, we focused on a class of social application that uses the users' actual location to provide personalised recommendation and allow for new interactions especially in urban settings. We have shown how these applications can expose their users to different privacy attacks that can be easily overlooked. We followed a formal framework to identify the classes of privacy violation to which users are subjected to without being aware of it and we have shown how these violations can all be carried out for the applications examined. This shows how using third party profiles to provide access to a specific applications may cause a security \emph{honey pot} for a possible attacker.

We also analysed web users tracking and introduced a set of metrics to show how information is sent to third-party tracking services when users surf the web. We also computed a set of network analysis on our graph model of the user online footprint. We were able to identify known trackers and isolate communities of similar trackers. This aspect is particularly interesting for the development of Privacy Enhancing Technologies for the web. Up to now, anti-tracking technologies have been built to simply stop third-party requests, alternative strategies might instead consider to send bogus information to certain over-connected tracker domains to masquerade the user real profile. At the same time a measurement of the average degree of the neighbourhood of a certain third-party domain can be used to evaluate how \emph{dangerous} this can be considered for the user's privacy.

We investigated the profile updates leading to the best and worst overall anonymity risk for a given activity and history. The analysis of our model was applied to an experimental scenario, using Facebook timeline data. Our main objective was measuring how privacy is affected when new content is posted. Often, a user of some online service is unable to verify how much a possible privacy attacker can find out about them. We used real Facebook data to show how our model can be applied to a real world scenario. This aspect is particularly important for content filtering in Facebook. In fact, as users are profiled on Facebook, the very same activity is used to filter the information they are able to access, based on their interests. There is no transparency on Facebook's side about how this filtering and profiling happens. We hope that studies like this might encourage users to seek more transparency in the filtering techniques used by online services in general.

We also introduced an example showing how it is possible to modify federated login mechanism to preserve users' privacy, to a certain extent. The example illustrated how web applications can be easily patched to provide users with a more privacy friendly experience. Up to now, in fact, users have been given the choice to either accept the terms of SPs and share all the data requested or not use the service. The model proposed instead would give users control on how they can share the data and present SPs with certain information regarding the users. 
Furthermore, SPs will not be given the possibility to access user information from the IdP as they please, but will have to request users to disclose certain information to the IdP. This way users will know exactly which information has been disclosed and in which interaction with SPs, hence giving them the choice to request for the data to be deleted if they wished to do so.

Given the extent of privacy issues and violations that are ignored by application developers and service providers, the author believes that the analysis, solutions and results presented in this dissertation provide the basis to understand these and possibly address them.
The author also hopes these results will motivate and provide a solid theoretical basis for additional analysis and privacy management techniques, and, ultimately, have a direct impact over users' privacy by eliminating or reducing barriers to the development of new and existing privacy aware protocols and services.