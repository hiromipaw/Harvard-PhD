\begin{savequote}[75mm] 
A Jedi uses the Force for knowledge and defense, never for attack.
\qauthor{Yoda - The Empire Strikes Back} 
\end{savequote}

\chapter{Conclusions and discussion}

\newthought{This dissertation examined a class of privacy issues for online communication}, proposing a model for the user identity and a possible new approach to information privacy management. This work focused on the analysis of privacy violation that can be found in different scenarios, on the web, on mobile applications and, more generally, on communication services.
One of our goals was to convince the reader that, as the web is shifting towards hypermedia data models and protocols, also privacy analysis and protection have to adopt the same mindset.

The motivation behind this work was understanding how data, created by users, flows between applications and services. A very powerful example in this field is the use of federated log in mechanisms. To register to a new social application, a user grants the service a certain level of access to their identity data, through, for example, their Facebook, Twitter or Google accounts, or simply by providing their email address and preferred username. Once the user grants access to their data, the application stores it and assumes control over how it is further shared. This data includes details about their offline identity, their whereabouts and in some situations even the company they work for. Identity providers offer login technologies, allowing the application to identify the user and receive precise information about them. The user will never be notified again on who is accessing their data, nor if this is transferred to third parties. We showed how this mechanism can be modified to mitigate or avoid this.

We believe that an important aspect of privacy protection is giving web users the possibility to control their digital footprints. More specifically, we are aware that privacy issues involve a plurality of complexities. This is especially true nowadays that privacy has acquired a completely different meaning because people conduct part of their existence through and on communication platforms. Privacy rights need to consider the implication of \emph{information privacy}, given that a person shares parts of their activities, interests and even thoughts with online service providers. As a consequence, the philosophical definition of privacy has evolved, while laws protecting individual privacy rights have tried to follow.

Up to now, in an online context, the right to privacy has commonly been interpreted as a right to \emph{information self-determination}. Acts typically claimed to breach online privacy concern the collection of personal information without consent, the selling of personal information and the further processing of that information. This definition of privacy breach can be considered valid until the user has direct control of the data they have created.

This work started by analysing information filtering systems. These systems have been developed to predict users' preferences, and eventually, use the resulting predictions for different services, depend on users revealing their personal preferences by annotating items that are relevant to them.  At the same time, by revealing their preferences online users are exposed to possible privacy attacks and all sorts of profiling activities by legitimate and less legitimate entities.

We showed how query forgery arises, among different possible PETs, as a simple strategy in terms of infrastructure requirements, as no third parties or external entities need to be trusted by the user in order to be implemented. However, query forgery poses a trade-off between privacy and utility. Measuring utility by computing the list of useful results that a user would receive from a recommendation system, we have evaluated how three possible tag forgery techniques would perform in a social tag application. With this in mind, a dataset for a real world application, rich in collaborative tagging information has been considered.

It was calculated how the performance of a recommendation system would be affected if all the users implemented a tag forgery strategy. We hence considered an adversary model where a passive privacy attacker is trying to profile a certain user. The user, in response, adopts a privacy strategy aiming at concealing their actual preferences, minimising the divergence with the average population profile. The results present a compelling outcome regarding how implementing different PETs can affect both user privacy risk, as well as the overall recommendation utility. We used a simple experimental evaluation, of a real world application scenario, to demonstrate how the performances degradation of a recommendation system, is small if compared to the privacy risk reduction offered by the application of these techniques.

Furthermore, we focused on a class of social application that uses the users' actual location to provide personalised recommendation and allow for new interactions, especially in urban settings. We confirm how these applications can expose their users to different privacy attacks that can be easily overlooked. We followed a formal framework to identify the classes of privacy violation to which users are subjected to without being aware of it and we have shown how these violations can all be carried out for the applications examined. This shows how using third party profiles to provide access to specific applications may cause a security \emph{honey pot} for a possible attacker.

We also analysed web users tracking and introduced a set of metrics to analyse and measure how advertising services track users on the web. We used the implicit connections between users profiles, tracking services and visited pages to compute a network analysis of the user online footprint. We were able to identify known trackers and isolate communities of similar trackers. This aspect is particularly interesting for the development of Privacy Enhancing Technologies for the web. Up to now, anti-tracking technologies have been built to simply stop third-party requests. Alternative strategies might instead consider sending bogus information to certain over-connected tracker domains to masquerade the user real profile. Furthermore, the graph analysis of the user's footprint provided an alternative method to evaluate how \emph{dangerous} a tracking network can be considered for the user's privacy.

Users' profiles also change over time, reflecting how real-world individuals change their tastes and preferences in comparison to, for example, a reference population. Every time new information is shared, the user is disclosing more about themselves or their social interactions, eventually changing their privacy risk. In this case, our main objective was measuring how privacy is affected when new content is posted. We considered the differential update of the anonymity risk of user profiles due to a marginal release of novel information, based on an information-theoretic measure of anonymity risk, precisely, the Kullback\hyph Leibler divergence between a user profile and the average population's profile.
We applied our model to the problem of algorithmic transparency in content filtering, by considering an experimental scenario based on real Facebook data. Users' profiles are, in fact, used on Facebook to filter the information they are able to access, based on their interests. There is no transparency on Facebook side about how this filtering and profiling happens. We hope that studies like this might encourage users to seek more transparency in the filtering techniques used by online services in general.

Finally, we introduced an analysis of how federated login exposes users to a set of privacy violations. We show with two examples how federated login mechanisms can be modified to preserve users' privacy, to a certain extent. The example illustrated how web applications can be developed to provide users with a more privacy friendly experience. Up to now, in fact, users have been given the choice to either accept the terms of service providers and share all the data requested or not use the service. The model applied in the practical examples, instead, would give users control on how data can be shared selectively. 
Furthermore, service providers will not be given the possibility to access user information from identity providers as they please. We suggest a mechanism that keeps the user in control of how and when novel information is shared. This way users will know exactly which information has been disclosed and when. 

Given the extent of privacy issues and violations that are ignored by application developers and service providers, the author believes that the analysis, solutions and results presented in this dissertation provide the basis to understand these and possibly address them.
The author also hopes these results will motivate and provide a solid theoretical basis for additional analysis and privacy management techniques, and, ultimately, have a direct impact on users' privacy by eliminating or reducing barriers to the development of new and existing privacy-aware protocols and services.