\begin{savequote}[75mm] 
Experience should teach us to be most on our guard to protect liberty when the government's purposes are beneficent. Men born to freedom are naturally alert to repel invasion of their liberty by evil-minded rulers. The greatest dangers to liberty lurk in insidious encroachment by men of zeal, well-meaning but without understanding.
\qauthor{Louis D. Brandeis} 
\end{savequote}

\chapter{Background and Related Work}

\newthought{Privacy issues} involve a plurality of complexities. The right to \emph{privacy} is a concept that has evolved over human history and has enclosed different other rights. A few centuries ago having a right to privacy meant protecting property rights, along with life and cattle. This right protected individuals from physical interference. At the end of the 19th century, it was assumed that the common law needed to guarantee the right of deciding to what extent the thoughts, sentiments and emotions of an individual could be communicated to third parties~\cite{warren1890right}.

Nowadays privacy has acquired a completely different meaning because people conduct part of their existence through and on communication platforms. Privacy rights need to consider the implication of \emph{information privacy}, given that a person shares parts of their activities, interests and even thoughts with online service providers. As a consequence, the philosophical definition of privacy has evolved, while laws protecting individual privacy rights have tried to follow.

\section{Privacy}
\noindent
The literature on privacy violation has struggled to agree on a definition of \emph{Privacy} considered its elusive nature. Yet, the right to privacy is considered one of the most fundamental rights for modern democratic societies, which also includes freedom of thoughts, control over a person body, protection of reputation and from indiscriminate search, interrogation and surveillance, control over personal information and right to solitude. Article 12 of the Universal Declaration of Human Rights~\cite{assembly1948universal}, states that \emph{"No one shall be subjected to arbitrary interference with his privacy"}. Article 18.4 of the Spanish constitution protects privacy and limits the use of information technology to safeguard personal intimacy of the citizens. In addition, the United States and a vast majority of nations also protect privacy in their constitutions and in laws. 

\subsection{A taxonomy of privacy}
\noindent
Privacy violations involve a multitude of activities, some of these harmful others problematic. In fact, personal computers and more generally communication devices that are carried around by people are capable of being located, identified and tracked across different locations, networks and services~\cite{michael2013location}. All these devices can, therefore, be used for a variety of surveillance activities, which are in itself detrimental to the user's interests. Until recently, in fact, the cost of surveillance and tracking of people and activities was proportional to the cost of directly reaching, asking or following a single person or a group of people. Technology, therefore, enhances the surveillance capabilities by introducing tools that allow the collection of information arising from a person's activities. This information can furthermore be combined and inferred, therefore offering a complete picture of that person. Daniel J. Solove in~\cite{solove2006taxonomy} defines a taxonomy of privacy to classify violation and understand privacy issues in a comprehensive and concrete manner. Following this approach privacy violations are classified in four main categories (Table ~\ref{tab:violations}). These are: 

\begin{enumerate}
 \item Information collection
 \item Information processing
 \item Information dissemination
 \item Invasion
\end{enumerate}
 
\subsubsection{Information collection:} 
Information collection results from activities such as surveillance, interrogation or information probing. It refers to actions aimed at watching, reading, listening, recording of individual activities or data about activities. It also refers to direct questioning of individuals or inference of information from data about them.
 
\subsubsection{Information processing:}
Information processing concerns the aggregation and identification of data. Failure to provide data security and the possibility for users to know who has accessed their data. This also includes secondary use of data to which the user has not been informed.
 
\subsubsection{Information dissemination:}
Information dissemination includes activity such as breach of confidentiality, unwanted disclosure and exposure of information. This also includes increased accessibility to individuals' information, appropriation and distortion of data about people. Information dissemination defines the very action of breaking the promise of keeping information confidential. It, therefore, implies actions aimed at the revelation of information about an individual that can change the image of that person within a group, including the appropriation of identity information and dissemination of false or misleading facts. 
 
\subsubsection{Invasion:}
Invasion is the threat of intrusion of an entity into someone private life and it includes acts that are said to disturb one tranquillity or solitude.

\begin{table*}[ht]
\centering
\caption{Classification of privacy violations}
\def\arraystretch{2.0}
\begin{tabular}{ | p{6em} || p{6em} || p{14em} |}
  \hline
   \textbf{Violation} & \textbf{Activities} & \textbf{Actions} \\
  \hline
  \hline
Collection    
    & - Surveillance; \newline
      - Information \newline
      probing; \newline
      - Interrogation.
    & - Watching, listening, recording of \newline 
        individuals' activities. \newline
      - Questioning individuals directly. \newline
      - Inferring information from data.\\
  \hline
Processing             
    & - Aggregation; \newline
      - Identification; \newline
      - Insecurity; \newline
      - Secondary \newline
        use; \newline
      - Exclusion.                        
    & - Gathering of data about individuals. \newline
      - Identification of physical identities \newline
        from online data. \newline
      - Carelessness in protecting data. \newline
      - Failure in allowing users to know \newline
        who has accessed to their data. \\
  \hline
Dissemination             
    & - Breach of \newline
        confidentiality; \newline
      - Disclosure; \newline
      - Exposure; \newline 
      - Increased \newline
        accessibility; \newline
      - Data appropriation; \newline
      - Distortion.
    & - Breaking the promise of keeping the information confidential. \newline
      - Revelation of information about an \newline
        individual that impacts the \newline
        way other see them. \newline
      - Appropriation of identity information. \newline
      - Dissemination of false or misleading \newline
        information. \newline
      - Transfer of personal data to third \newline
        party or threat to do so. \\
  \hline
Invasion                 
    & - Intrusion of \newline
        someone \newline
        private life.
    & - Acts that can disturb one tranquillity \newline
        or solitude. \\
  \hline 
\end{tabular}
\label{tab:violations}
\\[2.5pt]
 \begin{flushleft}
The table summarises the classification used to categorise privacy violation in proximity-based social application.
\end{flushleft}
\end{table*}

\subsection{Identifying privacy violation on social networks and applications}
\noindent
The classification of privacy violations introduced suggests that users should be particularly careful with the information they share on social networks and applications. It has been shown how leaking bits of personal information on one platform can be used for concrete privacy attacks. For example, physical identification and password recovery attacks can be based on the knowledge of personal information or the use of a known secret~\cite{irani-et-al}. It has been shown how the attribute set {birth-date, gender, zip code} poses concrete risks of individual identification~\cite{sweeney}, leading to details that can be used to identify physical persons or to infer answers to password recovery questions.

Another important aspect to consider is that the average online user joins different social networks with the objective to enjoy distinct services and features. On each service or application, an identity gets created, containing personal details, preferences, generated content and a network of relationships. The set of attributes used to describe these identities is often unique to the user. In addition, application or services sometimes require the disclosure of different personal information, such as email or full name, to create a profile. Users possessing different identities on different services, often use those to verify another identity on a particular application, i.e. a user will use their Facebook and LinkedIn profile to verify their account on the third service~\cite{paridhi-et-al}. A set of information required by one service could, in fact, add credibility to the information the user has provided for a second application, by demonstrating that certain personal details overlap, and by adding other information, like, for example, a set of shared social relationships.

The analysis of publicly available attributes in public profiles shows a correlation between the amount of information revealed in social network profiles, specific occupations or job titles and use of pseudonyms. It is possible to identify certain patterns regarding how and when users reveal precise information~\cite{chen-et-al}. Finally, aggregating this information can lead an attacker to obtain direct contact information by cross-linking the obtained features with other publicly available sources, such, for example, online phone directories.

A famous method for information correlation was presented by Alessandro Acquisti and Ralph Gross~\cite{acquisti-et-al}. Leveraging on the correlation between individuals' Social Security numbers and their birth date, they were able to infer people Social Security numbers by using only publicly available information.

Privacy attackers can also exploit loose privacy settings of a user's online social connections, taking advantage of how humans interpret messages and interact with one another~\cite{cryto-gram}, developing semantic attacks~\cite{Kumaragur-et-all}. Therefore, mechanism helping to promote coordinated privacy policies could be more efficient to count attacks~\cite{brown-et-all}. 

Accurate coordinated policy could also warn users of which third party application they authorise to access their data. Social networking platforms, in fact, expose users' privacy to possible attacks by allowing third party application that accesses their data to be able to replicate it. Sandboxing techniques could be implemented allowing users to share information among social relationships, while also helping third party application to securely aggregate data according to differential privacy properties~\cite{viswanath2012keeping}. 

Users should be allowed to choose an appropriate level of privacy for their needs and should be made aware of unwanted access to their data. This would permit protection of personal information that is being collected by mobile devices, including the derived inferences that could be drawn from the data. Semantic Web technologies can be implemented to specify high-level, declarative policies describing user information sharing preferences~\cite{jagtap2011preserving}.

A study on how users perceive the value of online and offline Personal Information (PI), shows that users value their PI related to their offline identities more (3 times) than what they willing share online~\cite{carrascal2013your}. This includes also valuing more information related to their financial transactions and social network interactions than other online activities like search and shopping. Studies of this kind show how users are probably unknowingly sharing online more than they intended and how tracking technologies implement methods that collect user data without informing the users. In fact, studies that have considered the users' perception of online advertising and the extent of online tracking have shown how the users' attitude generally changed when they found out that most of online advertising and therefore tracking activities happens without their consent~\cite{cranor2012can}

Users, in fact, consider three main deciding factors when consulted about how and to what extent they are willing to disclose personal and sensitive information, especially information about their location, to social relations~\cite{consolvo2005location}. These factors were: who was requesting a particular information, why that information was requested, and what level of detail would be most useful to the requester. 

This aspect of users' perception of sensitive information disclosure is particularly relevant when it has been shown~\cite{toch2013locality} that knowing a user location is used as a grounding mechanism in applications that lets users interact with their nearby. Geo-tagged information set the basis for a platform for honest and truthful signals in the process of forming new social relations.

At the same time, geolocalised information attached to users' activities can be used, by an attacker, to derive models of user mobility and provide data for context-aware applications and recommendation systems~\cite{melia-segui-et-al}. This information can also be used to cluster communities with different preferences and interests into different geographical communities~\cite{zhu-et-al}. 

Also, while some social networking applications use some form of obfuscation of the users' actual positions, precise location information can be still be derived. An attacker 
could use the partial information to identify a user's real position even when their exact coordinates are hidden or obfuscated by various  location hiding techniques~\cite{li2014all}.

While malicious attackers can target users, online services and platforms can also track their behaviour for a variety of purposes. Therefore, although there are certainly innumerable advantages in creating services that enable people to communicate so easily, it is as well important for users to retain control over which data they have been shared online over time. In the private sphere it has been said that "literally, Google knows more about us than we can remember ourselves." This situation has led to growing concerns regarding online privacy. In China, for example, one estimate suggests there are over 30.000~\cite{internet-freedom} government censors monitoring online information.

In addition to user-generated content, \emph{"metadata"} regarding this content, are collected and stored by public and private organisations. Metadata are descriptions of actual documents that can be easily read by a machine for a variety of uses, from searching and sorting to pattern recognition. This has lead in the last few years to the development of a new term to describe hyperlinked data objects: hyperdata. Hyperdata indicates data objects linked to other data objects in other places as hypertext indicates text linked to another text in other documents. Hyperdata enables the formation of a web of data, evolving from the "data on the web" that is not interrelated (or at least, not linked). Tools and information technology architectures employing visualisation and privacy enhancing technologies become, therefore, central to help users maintain a desired online footprint and retain a certain level of control over their data. At the same time, these tools can be useful to developers as well, to be aware of the possible privacy and security implication of their work.

\subsection{User profiling}
\noindent
With user profile we mean a container of an individual tastes, preferences and behaviour that can be used to predict future activities. A user's profile gives away the answer to whether or not that person can be interested in a certain product or service.

In recommendation systems employing tags or in any system allowing resource annotation, users decide to disclose personal data in order to receive, in exchange, a certain benefit. This earned value can be quantified in terms of the customised experience of a certain product~\cite{a01}. For such a recommendation system to work, and successfully propose items of interest, user preferences need to be revealed and made accessible partially or in full, and thus exposed to possible privacy attacks.

When a user expresses and shares their interests by annotating a set of items, these resources and their categorisation will be part of their activity. The recorded users' activities will allow the used platform to ``know more" about each of them, and therefore suggesting over time useful resources. These could be items similar to others tagged in the past, or simply close to the set of preferences expressed in their profile. In order to protect their privacy, a user could refrain from expressing their preferences altogether. While in this case, an attacker would not be able to build a profile of the user in question, it would also become impossible for the service provider to deliver a personalised experience: the user would then achieve the maximum level of privacy protection, but also the worst level of utility.

Various and numerous approaches have been proposed to protect user privacy by also preserving the recommendation utility in the context of social tagging platform. These approaches can be grouped around four main strategies~\cite{Shen07SIGIR}: encryption-based methods, approaches based on trusted third parties (TTPs), collaborative mechanisms and data-perturbative techniques. In traditional approaches to privacy, users or application designers decide whether certain sensitive information is to be disclosed or not. While the unavailability of this data, traditionally attained by means of access control or encryption, produces the highest level of privacy, it would also limit access to particular content or functionalities. This would be the case of a user freely annotating items on a social tagging platform. By adopting traditional PETs, the profile of this user could be made available only to the service providers but kept completely or partially hidden from their network of social connections on the platform. This approach would indeed limit the chances of an attacker profiling the user, but would, unfortunately, prevent them from receiving content suggested by their community.

A conceptually simple approach to protecting user privacy consists in a TTP acting as an intermediary or \emph{anonymiser} between the user and an untrusted information system. In this scenario, the system cannot know the user ID, but merely the identity of the TTP involved in the communication. Alternatively, the TTP may act as a \emph{pseudonymiser} by supplying a pseudonym ID' to the service provider, but only the TTP knows the correspondence between the pseudonym ID' and the actual user ID. In online social networks, the use of either approach would not be entirely feasible as users of these networks are required to authenticate to login. Although the adoption of TTPs in the manner described must, therefore, be ruled out, the users could provide a pseudonym at the sign-up process. In this regard, some sites have started offering social-networking services where users are not required to reveal their real identifiers. Social Number~\cite{SocialNumber} is an example of such networks, where users must choose a unique number as their ID.

Unfortunately, none of these approaches effectively prevents an attacker from profiling a user based on the annotated items content, and ultimately inferring their real identity. This could be accomplished in the case of a user posting related content across different platforms, making them vulnerable to techniques based on the ideas of re-identification. As an example, suppose that an observer has access to certain behavioural patterns of online activity associated with a user, who occasionally discloses their ID, possibly during interactions not involving sensitive data. The same user could attempt to hide under a pseudonym ID' to exchange information of confidential nature. Nevertheless, if the user exhibited similar behavioural patterns, the unlinkability between ID and ID' could be compromised through the exploitable similarity between these patterns. In this case, any past profiling inferences carried out by the pseudonym ID' would be linked to the actual user ID.

A particularly rich group of PETs resort to users collaborating to protect their privacy. One of the most popular is \emph{Crowds}~\cite{Reiter98ISS}, which assumes that a set of users wanting to browse the Web may collaborate to submit their requests. Precisely, a user wishing to send a request to a Web server selects first a member of the group at random, and then forwards the request to them. When this member receives the request, it flips a biased coin to determine whether to forward this request to another member or to submit it directly to the Web server. This process is repeated until the request is finally relayed to the intended destination. As a result of this probabilistic protocol, the Web server and any of the members forwarding the request cannot ascertain the identity of the actual sender, that is, the member who initiated the request.

We consider collaborative protocols~\cite{Domingo09DKE,Rebollo09COMCOM,Domingo12INS} like Crowds, not suitable for the applications addressed in this work although they may be effective in applications such as information retrieval and Web search. The main reason is that users are required to be logged into online social tagging platforms. That is, users participating in a collaborative protocol would need the credentials of their peers to login, and post on their behalf, which in practice would be unacceptable. Besides, even if users were willing to share their credentials, this would not entirely avoid profiling based on the observation of the resources annotated.

In the case of perturbative methods for recommendation systems,~\cite{Polat03SDM}~proposes that users add random values to their ratings and then submit these perturbed ratings to the recommender. When the system has received these ratings, it executes an algorithm and sends the users some information that allows them to compute the final prediction themselves. When the number of participating users is sufficiently large, the authors find that user privacy is protected to some degree, and the system reaches an acceptable level of accuracy. However, even though a user may disguise all their ratings, merely showing interest in an individual item may be just as revealing as the score assigned to that item. For instance, a user rating a book called "How to Overcome Depression" indicates a clear interest in depression, regardless of the score assigned to this book. Apart from this critique, other works~\cite{Kargupta03ICDM,Huang05SIGMOD} stress that the use of certain \emph{randomised} data-distortion techniques might not be able to preserve privacy completely in the long run.

In line with these two latter works,~\cite{Polat05SAC}~applies the same perturbative technique to collaborative filtering algorithms based on singular-value decomposition, focusing on the impact that their technique has on privacy. For this purpose, they use the privacy metric proposed by Agrawal, and Aggarwal,~\cite{Agrawal01SIGMOD}, effectively a normalised version of the mutual information between the original and the perturbed data, and conduct some experiments with datasets from Movielens~\cite{Movielens} and Jester~\cite{Jester}. The results show the trade-off curve between accuracy in recommendations and privacy. In particular, they measure accuracy as the mean
absolute error between the predicted values from the original ratings and the predictions obtained from the perturbed ratings.

The approach considered in this study follows the idea of perturbing the information implicitly or explicitly disclosed by the user. It, therefore, represents a possible alternative to hinder an attacker in their efforts to profile their activity precisely, when using a personalised service. The submission of false user data, together with genuine data, is an illustrative example of data-perturbative mechanism. In the context of information retrieval, query forgery~\cite{Rebollo10IT, parra2014optimal, Rebollo10IT, parra2012privacy} prevents privacy attackers from profiling users accurately based on the \emph{content} of queries, without having to trust the service provider or the network operator, but obviously at the cost of traffic overhead. In this kind of mechanisms, the perturbation itself typically takes place on the user side. This means that users do not need to trust any external entity such as the recommender, the ISP or their neighbouring peers. Naturally, this does not signify that data perturbation cannot be used in combination with other third-party based approaches or mechanisms relying on user collaboration.

Certainly, the distortion of user profiles for privacy protection may be done not only by means of the insertion of false activity but also by suppression. An example of this latter kind of data perturbation is the elimination of tags as a privacy-enhancing strategy~\cite{Parra10TB, rebollo2012query, parra2012optimal, Parra12TKDE}, applied in the context of the semantic Web. This strategy allows users to preserve their privacy to a certain degree, but it comes at the cost of a degradation in the semantic functionality of the Web. Precisely, the the privacy-utility tradeoff posed by the suppression of tags was investigated mathematically ~\cite{parra2012optimal, parra2014measuring, puglisi2015content}, measuring privacy as the Shannon entropy of the perturbed profile, and utility as the percentage of tags users are willing to eliminate. Closely related to this are also other studies regarding the impact of suppressive PETs~\cite{Parra12TKDE, puglisi2015content, parra2016shall}, where the impact of tag suppression is assessed experimentally in the context of various applications and real-world scenarios.

While PETs to protect user profiles have been introduced and implemented we also believe that the privacy and sensitiveness of the information becoming accessible to third parties can be easily overlooked. The problem of measuring user privacy in systems that profile users on the basis of the items they rate or tag is approached adopting a quantifiable measure of user privacy. Jaynes' rationale on maximum entropy methods~\cite{Jaynes57PRS1,jaynes1982rationale} was used to measure the privacy of confidential data modelled by a probability distribution by means of its Shannon entropy and Kullbach-Lieber divergence~\cite{rodriguez2015entropy, parra2014measuring}. This is particularly relevant when online services provide the users with the perception that sharing less data impact their optimal services experience. 

\section{Web tracking}
\noindent
Information regarding locations, browsing habits, communication records, health information, financial information, and general preferences regarding user online and offline activities are shared by different parties. This level of access is often directly granted from the user of such services. In a wide number of occasion though, private information is captured by online services without the direct user consent or even knowledge. We believe that the privacy and sensitiveness of the information becoming accessible to third parties can be easily overlooked. 

To personalise their services or offer tailored advertising, web applications use tracking services that identify a user through different networks~\cite{veeningen2014line,getoor2012entity}. These tracking services usually combine information from different profiles that users create, for example, their Gmail address or their Facebook or LinkedIn accounts. In addition, specific characteristics of the user's devices can be used to identify them through different sessions and websites, as described by the Panopticlick project~\cite{eckersley2010unique}.

Browser fingerprinting is a technique implemented by analytics services and tracking technologies to identify uniquely a user while they browser different websites. Different features of a specific browser setup can be used to identify uniquely a user. Supported languages, browser extensions or installed fonts~\cite{boda2012user} can be used to identify a browser setup among others. More advanced techniques distinguish between browsers' JavaScript execution characteristics~\cite{mowery2011fingerprinting}. These features are particularly interesting since they are more difficult to simulate or mitigate in practice. Targeting JavaScript execution characteristics actually means looking at the innate performance signature of each browser's JavaScript engine, allowing the detection of browser version, operating system and micro-architecture. These attacks can also work in situations where traditional forms of system identification (such as the user-agent header) are modified or hidden. Other techniques exploit the whitelist mechanism of the popular NoScript Firefox extension.This mechanism allows the user to selectively enabling web pages' scripting privileges to increase privacy by allowing a site to determine if particular domains exist in a user's NoScript whitelist.

It is important to note that while tracking creates serious privacy concerns for Internet users, the customisation of results is also beneficial to the end user~\cite{castelluccia2012behavioural}. In fact, while tailored services offer to the user only information relevant to their interests, it also allows some companies and institutions to concentrate an enormous amount of information about Internet users in general.~\cite{rao2015they} investigate user profiling and access mechanisms offered by online data aggregator to users' collected data. Both the collected data and its accuracy was analysed together with the user's concerns. In their findings, about 70\% of the participants to the study expressed some concerns about the collection of sensitive data, its level of detail and how it might be used by third parties, especially for credit and health information.

Generally speaking, the activity of tracking a user across different websites, visits and devices, involves three main actors: the user, the tracking network, the list of websites visited. Every time a user visits a website a piece of code on the page is called asynchronously from the user's browser. When the call to the tracking network is performed a number of user data is transferred and used to profile the user at a later time and/or on a different website or device.  By modelling the user behaviour as a directed graph, it is possible to uncover the underlying network structure of the user footprint and the tracking networks tracking the user across the web~\cite{kalavri2016like}~\cite{schelter2016tracking}.

It has been shown how most successful tracking networks exhibit a consistent structure across markets, with a dominant connected component that, on average, includes 92.8\% of network vertexes and 99.8\% of the connecting edges~\cite{gomer2013network}.~\cite{gomer2013network} have measured the chance that a user will become tracked by all top 10 trackers in approximately 30 clicks on search results to be of 99.5\%. More interesting,~\cite{gomer2013network} have shown how tracking networks present properties of the small world networks. Therefore, implying a high-level global and local efficiency in spreading the user information and delivering targeted ads.

It is interesting to note that the behaviour of tracking networks follows that of telemarketing operations of the 80s and 90s. In~\cite{hoofnagle2012behavioral} the authors present an analysis of the history of telemarketing from cold calling potential customers on the phone, to the modern web tactics of tracking them across their browsing activities. It is particularly relevant how they point out that although users can try to avoid some modern communication tracking techniques, it is not guaranteed to assume that advertisers will respect individuals' choices and will not try to find alternative methods. In the past, technologies adopted to avoid sales calls were circumvented through clever new approaches by telemarketers. In 2010 in fact, the Wall Street Journal presented a series of articles on monitoring~\cite{angwin2010web}, stating how the "nation's 50 top websites on average installed 64 pieces of tracking technology onto the computers of visitors, usually with no warning."

An interesting property of networks to understand their architecture is the behaviour of the average degree of nearest neighbours~\cite{barrat2004architecture}~\cite{pastor2001dynamical}. The average degree of the nearest neighbours of a node $k_{nn}(k)$ is a quantity related to the correlations between the degree of connected vertices~\cite{maslov2002specificity}, since it can be expressed as the conditional probability that a given vertex with degree $k$ is connected to a vertex of degree $k'$. This property defines if the network in consideration is assortative if $k_{nn}$ is an increasing function of k or dissortative~\cite{newman2002assortative} if it is not. The property of assortativity has been used in the field of epidemiology, to help understand how a disease or cure spreads across a network. It is particularly interesting to note that assortativity can give a measurement if the removal of a set of network's vertices may correspond in curing, vaccinating or quarantining individual cells in the network. 

Another interesting aspect of networks is the presence of \emph{communities}. A common activity when analysing large network is to start finding communities by dividing the nodes into \emph{modules}. A common approach applies \emph{generative models} able to infer the model parameters directly from the data. A simple generative process is the Stochastic Block Model (SBM)~\cite{holland1983stochastic}. A stochastic block model is able to explicitly describe the global structure of a network, providing a model of how the network can be partitioned into subgroups (blocks) and how the probability distribution of the connections between the nodes (i.e. probability that a node is connected to another) depends on the blocks to which the nodes belong~\cite{faust1992blockmodels}.

The microcanonical formulation~\cite{peixoto2012entropy} of blockmodels takes as parameters the partition of the nodes into groups $b$ and a $B \times B$ matrix of edge counts $e$, where $e_{rs}$ is the number of edges between groups $r$ and $s$. Since edges are then placed randomly, nodes belonging to the same group possess the same probability of being connected with other nodes of the network. Furthermore, to be able to find small groups in large network nested SBM are used. With nested SBM groups are clustered into groups, and the matrix $e$ of edge counts are generated recursively from another SBM~\cite{peixoto2014hierarchical}. Agglomerative multilevel Markov chain Monte Carlo (MCMC) algorithm as described in~\cite{peixoto2014efficient}, can be implied to compute a partition of the resulting graph.

Protection techniques against tracking networks are implemented through software agents able to identify if third-party requests are accessing private data. These agents include Privacy Badger~\cite{privacy-badger}, Mozilla Lightbeam~\cite{lightbeam}, Ghostery~\cite{ghostery}, AdBlock~\cite{adblock}, and so on. Some of these agents block certain JavaScript functions or attempts to access determined browser functionality that can be used to uniquely identify the user. Some others implement a Tracking Protection Lists (TPL). A TLP can be seen as a blacklist of identified tracking domains that user might want to block. 

Another interesting aspect of advertising services is how they are designed to work on feedback loops~\cite{degeling2016your}. An advertising service can, in fact, be seen as a black-box providing the tracker trying to identify or profile the user, and the returned advertising content. The tracker is used to send information back to the advertising service, which in response will return a certain content tailored to the user preferences. Within this feedback loop, different aspects of the user behaviour are taken into consideration. These include certainly the users browsing history and their click through rate, i.e. a measurement of the amount of time users in a population are more likely to interact with an ad. In more sophisticated advertising solution also user social connections are taken into consideration.

Advertising, therefore, services raise the problem of confidentiality of the user reading activity~\cite{ard2013confidentiality}. Up to know an eloquent example of this problem was provided by the way public library in the US operates. Reading activities were considered historically private and were protected through a set of rules that restricted libraries ability to exploit reading records. This regime is clearly bypassed when libraries decide to provide digital services to their users. Digital services providers and third parties can, in fact, access users reading activities without agreeing to the library confidentiality regime.

\section{online footprints}
\noindent
As users spend time online they produce private information across a multitude of services. These are web and mobile apps, websites, different platforms, social media, mobile and Internet of Things (IoTs) devices. Furthermore, data shared with one platform can be then shared with third-parties without the user having to consent again. The notion of secondary privacy diffusion was introduced to describe when user data are either deliberately transmitted or inadvertently leaked to a third-party~\cite{krishnamurthy2009privacy}. Examples of secondary privacy diffusion in today's web are numerous. Imagine a scenario where a user is setting up their mobile phone for the first time. When they configure the device, all their data is transferred to various service providers. Among this data are also contact details of other people. Some of these people might have gone a long way trying to protect their details from disclosure, nor have they consented to their communications and information to be sent and stored by a third-party. 

Different projects have tried to capture how services track users across websites, applications and devices, some of these are: Mozilla Lightbeam~\cite{lightbeam}, which allow users to visualise how web trackers are connected to the websites they visit, Facebook-Tracking-Exposed~\cite{FTE}, a project aiming at increase transparency behind personalization algorithms and expose how Facebook filtering works, Data Selfie, a browser extension that tracks users on Facebook to show their data traces and reveal how machine learning algorithms the very same data to gain insights about their profile~\cite{data-selfie}.

Hyperdata represents the evolution of the web as we know now. When Tim Berners-Lee envisioned the semantic web in 2001~\cite{SciAMMag}, the web of data was described as a framework where autonomous agents could access structured information and conduct automated reasoning. These agents can be imagined as interconnected services accessing streams of data through a set of protocols or interfaces. APIs can provide such interfaces by specifying how software components can interact with each others through one or more protocols. When a request is sent to an individual service through an API, a stream of data is obtained as a reply. This reply is expressed in a format that can be parsed and interpreted. A hypermedia API would additionally specify links between the data object returned; therefore, a hypermedia browser would be able to explore such flow of information as web browser can navigate through the hyperlink in a web page.\\

Secondary data leakages are in reality a by-product of the way the web works. Data on the web is consumed in the form of objects, like documents, or simple snippets of data, linked to other objects. These objects are often referred to as hyperdata. Hyperdata can be easily explained by considering it as an evolution of the hypertext. Within a hypertext document, in fact, paragraphs composing the document could be linked to some other text in the same or a different location. Hyperdata objects instead are either consumed through an Application Programming Interface (API), specifying how the different software components should interact with each other's or also embedded into existing document.\\

Examples of hyperdata are markup standards like Microformats, Microdata and RDFa used by websites to embed structured data to describe products, services, events, and make user information available already into their HTML pages~\cite{bizer2013deployment}. A microformat (sometimes abbreviated $\mu F$) is an approach to describe data in a way that can be understandable both to machines and to humans. It builds on top of existing standards, and it is used to include metadata or other attributes into existing web pages or RSS feeds. This way software agents can process information that would otherwise be readable only for humans, such as contact information, geographical coordinates, or calendar events.\\

When hyperdata objects are explored through an API, this would probably implement different communication protocols to allow several technologies to access independently to hyperdata objects. To enable this exchange of information among heterogeneous systems, the API can implement a language-neutral message format to communicate. This could be the case of XML or JSON languages, used as containers for the exchanged messages. In this extent, an 'Hypermedia API' is one that is designed to be accessed and explored by any device or application. Its architecture is hence similar to the structure of the web and the same reasoning when serving and consuming the API it is applied. 

The response data for any API call can be returned in the desired format. Most RESTful services return either XML or JSON, while some give the options to choose a preferred format. The format is defined either in the request header or the URI called. It is also possible to set the default format that is returned unless another format is specified.

JSON stands for JavaScript Object Notation, and it is defined as a lightweight data-interchange format. It has been based on a subset of the JavaScript Programming Language, Standard ECMA262 3rd Edition December 1999. JSON is a language to exchange data, so it is defined as language independent format and easy to be read by humans as well as being parsed by programs. A JSON object is a collection of name/value pairs, like a dictionary data structure in python or a hash in ruby. An object begins with \{ (left brace) and ends with \} (right brace). Each name is followed by : (colon) and the name/value pairs are separated by , (comma). JSON also supports ordered lists. These can be seen as a list of values, as in an array. An array begins with [ (left bracket) and ends with ] (right bracket). Values are separated by , (comma). A value can be a string in double quotes, or a number, or true or false or null, or an object or an array. These structures can be nested (Table: ~\ref{tab:json}).

\begin{table*}[ht]
\centering
\begin{tabular}{| l |}
\hline
\{\\
"products": [\\
\{ "type":"Sneakers" , "brand":"Adidas" \}, \{ "type":"Runners" , "brand":"Nike" \},\\
\{ "type":"Accessories" , "brand":"Puma" \}\\
]\\ \}\\
\hline
\end{tabular}
\caption{A JSON example}
\label{tab:json}
\end{table*}

XML stands for Extensible Markup Language, and it is designed as a language to define a set of rule to encode documents in a format that is both human-readable and machine-readable. It is defined in the XML 1.0 Specification produced by the W3C. XML was created to structure, store, and transport information, so this is why it is so handy and straightforward to use for application to communicate with each others. With XML, it is possible to define the tags, attributes and nesting rules that make a document valid according to a particular document type definition (DTD) or XML schema (XSD, XML Schema Definition), according to the application-specific choices. A DTD is a set of markup declarations that define a document type, while an XML schema expresses a set of rules to which an XML document must conform in order to be considered 'valid' according to that schema (Table: ~\ref{tab:xml}).
\\
\begin{table*}[ht]
\centering
\begin{tabular}{| l |}
\hline
$<product>$\\
  $<type>Sneakers</type>$\\
  $<brand>Adidas</brand>$\\
$</product>$\\
\hline
\end{tabular}
\caption{An XML example}
\label{tab:xml}
\end{table*}

To protect data collected by third-parties and preserve the confidentiality of users' footprints a privacy framework around the concept of "virtual walls" was proposed in~\cite{kapadia2007virtual}. A virtual wall extends the notion of real world privacy provided by a closed room, sheltering a person from the outside world. A virtual wall would be a set of user specified policies controlling access to all their personal data
in a way that is as intuitive and consistent with their notion of physical privacy.

A common problem for user footprints protection tools has been identified in the user attitude towards disclosing new information and their awareness, or lack-of-there-of, regarding possible data leakage. These aspects are amplified by the economics of web services based on advertising. It has been shown though, that an efficient client-side tool that maximises users' awareness over their online footprint can help users making informed decisions
over how they disclose new data~\cite{malandrino2013privacy}.

Different approaches for data management have also been proposed using cryptographic techniques. \emph{Anonrep}~\cite{zhai2016anonrep} is an anonymous reputation system where users anonymously post messages and tag them with their reputation score, without revealing other sensitive information. AnonRep reliably tallies other users' feedback (e.g., likes or votes) without leaking the user identity or the exact reputation score, and also maintaining a level of security against duplicate feedback and score tampering. Smart contracts based on the concept of decentralised crypto-currencies can facilitate data transactions and service management between individuals, applications and devices. In the field of smart contracts, Hashcash~\cite{back2002hashcash, back2002hashcash2} was probably the first of such systems. Hashcash proposes a CPU cost function to compute a token that can be used as a proof-of-work. This concept introduced by Hashcash, together with previous ideas from other systems as e-cash and b-money, create the basis for a cryptocurrency. Bitcoin~\cite{nakamoto2008bitcoin} uses and expands these ideas to define a cryptographically secure mechanism to reach consensus over a series of cryptographically signed financial transactions. Bitcoin can be considered the first decentralised transaction ledger. Bitcoin itself has been forked several times and different version of the crypto-coin have been created introducing a number of variations over the protocols used~\cite{tschorsch2015bitcoin}~\cite{sprankel2013technical}. Other projects instead re-purpose core paradigms of Bitcoin to different applications and domains.

The Ethereum project builds upon previous work on the usage of a cryptographic proof of computational expenditure as a means of transmitting a value signal over the Internet~\cite{buterin2014next}. in Ethereum the Bitcoin ledger is considered as a state transition system. The current state in Bitcoin is the collection of all unspent transaction outputs (UTXO) with each UTXO having a denomination and an owner (defined by an address of a given length which can be considered as a cryptographic public key). A state transition function takes the current state and a transaction as inputs, and the new resulting state as output. This is similar to the standard banking system where the state is the balance sheet, a transaction is a request to move a sum of money $X$ from A to B, and the state transition function is the mechanism reducing the value in A's account by $X$ and incrementing the value in B's account by $X$.
Moreover, UTXO in Bitcoin can be owned not just by a public key but also by a more complicated script. Scripts in Bitcoin are expressed through a stack-based programming language allowing simple operations. With this paradigm, a transaction spending in UTXO must provide data satisfying the script. Likewise, the basic public key ownership mechanism of Bitcoin is implemented via scripts. In this case, the script takes an elliptic curve signature as input, verifies it against the transactions and address owning the UTXO and return 1 for success and 0 otherwise. More complicated scripts can be created for different purposes, allowing a decentralised cross-cryptocurrency exchange. Bitcoin scripting capabilities are however quite limited. The lack of Turing completeness and different states are a drawback to building more complex applications on top of the Bitcoin paradigm.
Ethereum provides a blockchain with a Turing-complete programming language. A computer program that runs on the blockchain is a contract. It consists of program code, storage file and account balance. A contract is created by posting a transaction to the blockchain. Once created the program code of a contract is fixed, and its code executed whenever it receives a message, either from a user or from another contract. This concept has been used to define the decentralised autonomous organisation and trust~\cite{slockit}~\cite{kosba2016hawk}.

In the field of the Internet of Things (IoTs), a number of techniques have been proposed. An interesting research effort in anonymous authentication systems is EPID~\cite{ruan2014privacy}. EPID is technology for active anonymity aiming at solving the problems of authentication, anonymity and revocation with finite field arithmetic and elliptic curve cryptography (ECC). In the EPID ecosystem three entities are defined: the authority responsible for generating, signing and revoking keys, the platform device receiving a service, the verifier that provides the service to the device. EPID provides a solution for a device to authenticate itself anonymously to a service provider. The defined protocol is one-way because the service provider is not authenticating back to the platform. 

An extension of EPID, ChainAnchor~\cite{hardjono2016cloud}, uses the blockchain as a mechanism to anonymously register device commissioning and decommissioning.ChainAnchor provides a privacy-preserving technique for device commissioning and assurance to service providers that the device is a genuine product issued by the manufacturer. Another blockchain-based approach proposes a combination of blockchain and off-blockchain storage instead. This combination is used to construct a privacy-focused personal data management platform~\cite{zyskind2015decentralizing}. With a decentralised approach, users are not required to blindly trust any third-party and are always aware of how their data is being managed and used. In addition, the blockchain recognises data ownership to the user, and not to the company providing the service.

The blockchain has also be used to extend the GPG approach to the \emph{web of trust}~\cite{wilson2015pretty, cryptoeprint:2016:469}, providing an alternative certificate format based on Bitcoin which allows a user to verify a PGP certificate using Bitcoin identity-verification transactions. The user will be able to form first degree trust relationships that are tied to actual values. Furthermore, the blockchain approach can also be used to design a novel distributed PGP key server and store and retrieve, to and from the ledger, Bitcoin-Based PGP certificates.  

Certcoin is a Public key infrastructure (PKIs) with no central authority~\cite{fromknecht2014decentralized} leveraging the consistency guarantees provided by cryptocurrencies such as Bitcoin and Namecoin to build a PKI that ensures identity retention, effectively preventing one user from registering a public key under another's already-registered identity.

Other digital identities management techniques have been built on top of common cross-site authentication schemes such as OAuth and OpenID. An example of such approach is Crypto-Book~\cite{maheswaran2013crypto} an approach which extends existing digital identities through public-key cryptography and ring signatures. A similar technique is proposed by UnlimitID~\cite{isaakidis2016unlimitid} a method for enhancing the privacy of common mechanisms for authorization and authentication, such as OAuth.