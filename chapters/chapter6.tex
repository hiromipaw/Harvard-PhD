\begin{savequote}[75mm]
Cyberspace. A consensual hallucination experienced daily by billions of legitimate operators, in every nation, by children being taught mathematical concepts... A graphic representation of data abstracted from banks of every computer in the human system. Unthinkable complexity. Lines of light ranged in the nonspace of the mind, clusters and constellations of data. Like city lights, receding...
\qauthor{William Gibson, Neuromancer}
\end{savequote}

\chapter{An information\hyph theoretic model for measuring the anonymity risk in time\hyph variant user profiles}

\newthought{Websites and applications use personalisation services} to profile their users, collect their patterns and activities and eventually use this data to provide tailored suggestions. User preferences and social interactions are therefore aggregated and analysed. Every time a user publishes a new post or creates a link with another entity, either another user, or some online resource, new information is added to the user profile. Exposing private data does not only reveal information about single users' preferences, increasing their privacy risk, but can expose more about their network that single actors intended. This mechanism is self-evident in \emph{social networks} where users receive suggestions based on their friends' activities.

This chapter is centred on an information-theoretic approach to measure the differential update of the anonymity risk of time-varying user profiles, continuing on previous work published in ~\cite{puglisi2017anonymity}. We are interested to measure how privacy is affected when new content is posted and how much third-party services \emph{get to know} about the users when a new activity is shared. We use actual Facebook data to show how our model can be applied to a real-world scenario.

\section{Background}
\noindent
Personalisation and advertising services collect user's activities to provide tailored suggestions. This data contributes to form over time what is considered the user online footprint. With the term online footprint we include every possible trace left by individuals when using communication services. It follows that the same notion of digital footprint spans all layers of the TCP/IP model, depending on the type of data taken into considerations. It is also important to note that the digital footprint of an individual is formed by their interaction with their social relationships, not only by their singular actions on a medium or platform. 

We can therefore consider users' online footprints as linked data, where each event generated by a single user includes information regarding other users but also regarding other events and entities. This way of considering online footprints is very similar to the very structure of the Web, where web pages link to other pages when they reference a certain individual or object. This social and interconnected aspect of digital footprints is particularly evident for services like Facebook~\cite{facebook}, where users are suggested new pages and social connections based on their friends' network of relationships and expressed preferences, or \emph{likes}.

Users' profiles also change over time, reflecting how real-world individuals change their tastes and preferences in comparison to, for example, a reference population. Every time new information is shared, the user is disclosing more about themselves or their social interactions, eventually changing their privacy risk.

More importantly, users tend to share their data and access to their identity accounts, such as Google~\cite{google} or Facebook~\cite{facebook}, when interacting with third\hyph party applications. These applications use federated log in mechanisms through the user's identity account. To use the application, users grant it a certain level of access to their private data through their profile. This data includes details about their real \emph{offline} identity, their whereabouts and in some situations even the company they work for. Once it has gained access, the application can now store user data and assume control over how it is further shared.  The  user  will never be notified again about who is accessing their data, nor if these are transferred to third parties. 

This aspect of privacy protection is particularly relevant since the right to privacy is commonly interpreted as the user's right to prevent information disclosure. When a user shares some content online, they are actively choosing to disclose some of their profile. At the same time, though, they might give away more that they intended, since no information is shared from app and service about how the profile is analysed or how the user's data is further shared. 

Online services ask the user to access certain information, yet no concrete information is passed on how the data will be used or stored. Furthermore, these services are often designed as mobile applications where all the devices installing the  app communicate with a centralised server and constantly exchange users' information, eventually allowing for unknown third parties, or potential attackers, to fetch and store this data. In addition, this information is often shared with insecure communication through the HTTP protocol, making it possible for a malicious entity to intercept these communications and steal user data. 

In this model the management of privacy and trust of the platform to which users handle their data is highly centralised. The user entrusts the service with all their data, often as part of a service agreement. Generally a few services control the market and therefore can inevitably \emph{know more} about the users. This is the case of popular email or messaging services, but also social networks, relationship apps and so on. These entities can easily know who is talking to whom and sometimes also the topic of their conversations.

In this chapter we analyse user online footprints as a series of events belonging to a certain individual. Each event is a document containing different pieces of information. An event correspond to an action generated by the user or one of their devices. When a user visits a website or creates a post on a blog, an event is created. We can think of an event as a hypermedia document, i.e., an object possibly containing graphics, audio, video, plain text, and hyperlinks. We call the hyperlinks selectors, and we use them to build the connections between the user's different identities or events. Each identity can be a profile or account that the user has created onto a service or platform, or just a collection of events, revealing something about the user. With account we mean an application account or a social network account, such as their LinkedIn or Facebook unique IDs. 

When the user decides to share some new content, or subscribes a service by sharing part of their profile data, novel information is released. This information is either made public or shared to a group of people, like for a new social network post, or it is rather shared to a third party app.

We are interested to measure the differential update of the anonymity risk of user profiles due to a marginal release of novel information, based on an information-theoretic measure of anonymity risk, precisely, the Kullback\hyph Leibler divergence between a user profile and the average population's profile.

We particularly considered real data shared by Facebook users as part of the Facebook-Tracking-Exposed project~\cite{FTE}. For the purpose of this study, we considered categorised Facebook posts. We imagined that an attacker is interested in capturing users' preferences by looking at their posts and imagined a scenario where the information shared through a new event (i.e.  sharing new content) increases or decreases the user's privacy risk, in other words, how much an attacker knows about them, once they have captured the new information.

In this work, we build upon a recent information-theoretic model for measuring the privacy risk incurred in the disclosure of a user's interests though online activity. Among other refinements, we incorporate an aspect of substantial practical importance in the aforementioned model, namely, the aspect of time-varying user profiles.

More precisely, we propose a series of refinements of a recent information-theoretic model characterising a user profile by means of a histogram of categories of interest, and measuring the corresponding privacy risk as the Kullback-Leibler divergence with respect to the histogram accounting for the interests of the overall population. Loosely speaking, this risk may be interpreted as an anonymity risk, in the sense that the interests of a specific user may diverge from those of the general population. Our main contributions are as follows.
 
\begin{itemize}
\item We preface our main analysis with an argument to tackle populations in which the distribution of profiles of interest is multimodal, that is, user profiles concentrate around distinguishable clusters of archetypical interests. We suggest that said information-theoretic model be applied after segmentation of the overall population according to demographic factors, effectively extending the feasibility of the original, unimodal proposal.

\item But the most important refinement and undoubtedly the main focus of this chapter consists in the extension of the aforementioned model to time-varying user profiles. Despite the practical significance of the aspect of time in the analysis of privacy risks derived from disclosed online activity, it is nevertheless an aspect all too often neglected, which we strive to remedy with this preliminary proposal. Here, the time variation addresses not only changes over time in the interests of a user, construed as a dynamic profile, but also novel activity of a possibly static profile, in practice known only in part.

\item The changes in anonymity risk are formulated as a gradient of the Kullback\hyph Leibler divergence of a user profile reflecting newly observed activity, with respect to a past history, and are inspired in the abstract formulation of Bregman projections onto convex sets, whose application to the field of privacy is, to the best of our knowledge, entirely novel.

\item For a given activity and history, we investigate the profile updates leading to the best and worst overall anonymity risk, and connect the best case to the fairly recent information-theoretic framework of optimised query forgery and tag suppression for privacy protection.

\item We contemplate certain special cases of interest. On the one hand, we provide a corollary of our analysis for the special case in which the anonymity risk is measured as the Shannon entropy of the user profile. On the other hand, we particularise our model in the extreme case in which the new observation consists in a single sample of categorised online activity.

\item Last but not least, we verify and illustrate our model with a series of examples and experiments with both synthetic and real online activity.

\end{itemize}

\section{An information\hyph theoretic model for measuring anonymity risk}
\label{sec:1-1}

\noindent
In this section, we build upon a recent information-theoretic model for measuring the privacy risk incurred in the disclosure of a user's interests though online activity. Among other refinements, we incorporate an aspect of substantial practical importance in the aforementioned model, namely, the aspect of time-varying user profiles.
 
Consider a user profile $p$, together with an average population profile $q$, both represented as histograms of relative frequencies of online activity along predefined categories of interest $i=1,\dots,m$. In the absence of a specific statistical model on the frequency distribution of user profiles, as argued extensively in~\cite{Rebollo10IT,Rebollo11SecTech,Parra14FGCS} on the basis of Jaynes' rationale for maximum entropy methods, we assume that \emph{anonymity risk} may be adequately measured as the \emph{Kullback\hyph Leibler} (KL) \emph{divergence} $\mathrm{D}(p\|q)$ between the user profile $p$ and the population's~$q$. The idea is that user profiles become less common as they diverge from the average of the population. Precisely, we define anonymity risk as

\begin{equation*}
\mathcal{R}\defeq\mathrm{D}(p\|q)\defeq\sum_{i=1}^m p_i\log \frac{p_i}{q_i}.
\end{equation*}

Usually, the basis of logarithm is 2 and the units of the divergence are bits.

Intuitively, the empirical histogram of relative frequencies (or type) $t$ of $n$ independent, identically distributed drawings should approach the true distribution $\bar{t}$ as $n$ increases. Those drawings may be loosely interpreted as sequences of online queries according to some underlying user interests represented by~$\bar{t}$. More technically, the extension of Jaynes' approximation to KL divergences for a sequence of independent events shows that the probability $p_T(t)$ of the empirical distribution $t$ is related to the KL divergence $\mathrm{D}(t\|\bar{t})$ with respect to the true distribution $\bar{t}$ by means of the limit
\begin{equation*}
-\tfrac{1}{n}\log p_T(t)\xrightarrow[n\to\infty]{}\mathrm{D}(t\|\bar{t}).
\end{equation*}

According to this model, the user profile $p$ plays the role of the empirical distribution $t$, and the population's profile $q$, the role of the true distribution~$\bar{t}$. In a way, we construe a user profile as an empirical instantiation of the population's profile. Concordantly, the divergence $\mathrm{D}(p\|q)$ between the user profile $p$ and the population's $q$ is a measure of how rare $p$ should be, which we regard in turn as a measure of \emph{anonymity risk}. The argument that the rarity of a profile may also be understood as a measure of how sensitive a user profile may be considered, offers a measure of \emph{privacy risk}. Admittedly, this model is limited to applications where the underlying assumptions may be deemed adequate, particularly when no specific, possibly multimodal distribution of the user profiles is available.

Another helpful interpretation of this measure stems from rewriting the user profile as a distribution $p_{I|J}$ of a random variable $I$ indexing online activity into predefined categories $i=1,\dots,m$, conditioned on the user identity $J$, defined on the user indexes $j=1,\dots,n$. Observing that the population profile is the expectation across all user profiles,
\begin{equation*}
q_I=\mathrm{E}_J\:p_{I|J}(\cdot|J),\quad\text{(more explicitly},\quad q_I(i)=\tfrac{1}{n}\sum_{j=1}^n p_{I|J}(i|j)\quad\text{for all}\ i\text{)},
\end{equation*}
we immediately conclude that the expected risk is
\begin{equation*}
\mathrm{E}_J\:\mathcal{R}(J)=\mathrm{E}_J\:\mathrm{D}\left(p_{I|J}(\cdot|J)\,\middle\|\,q_I\right)=\mathrm{I}(I;J),
\end{equation*}
namely, the mutual information between the online activity $I$ and the user identity~$J$.

\subsection{Multimodality of the KL divergence model and conditioning on demography}
\label{sec:1-2}

\noindent
Perhaps one of the major limitations of the direct application of the KL divergence model for characterising the anonymity of a profile is made clear when the distribution of profiles is concentrated around several predominant modes, contradicting the implicit unimodal assumption revolving around the population's profile~$q$. Intuitively, one may expect several clusters in which profiles are concentrated, corresponding to various demographic groups, characterised by sex, age, cultural background, etc.

In order to work around this apparent limitation, we may simply partition the data into a number of meaningful demographic groups, indexed by $k$, and calculate the average population profile $q_{I|K}(\cdot|k)$ for each group~$k$. Then, redefine the demographically contextualised anonymity risk as the KL divergence between the profile $p_{I|J}(\cdot|j)$ of user $j$, in group $k(j)$, and the corresponding reference $q_{I|K}(\cdot|k(j))$, that is,
\begin{equation*}
\mathcal{R}_\text{context}(j)\defeq\mathrm{D}\left(p_{I|J}(\cdot|j)\,\middle\|\,q_{I|K}(\cdot|k(j))\right).
\end{equation*}
Obviously, the model will be suitable as long as the profile distribution is unimodal within each demographic context, in the absence of a more specific model. Note that the measure of anonymity risk of the disclosed interests is now conditioned on demographic data potentially observable by a privacy attacker.

\subsection{Gradient of the KL divergence and information projection}
\label{sec:1-3}

\noindent
Before addressing the problem of the differential update per se, we quickly review an interesting result on the gradient of the KL divergence, and its application to convex projections with said divergence. Directly from the definition of the KL divergence between distributions $p$ and $q$ for a general logarithmic basis, compute the gradient on the first argument
\begin{equation*}
\nabla_p\mathrm{D}(p\|q)={\left(\log \frac{p_i}{q_i}+\log e\right)}_i.
\end{equation*}
Swift algebraic manipulation shows that
\begin{equation} \label{eq:gradient}
\mathrm{D}(p\|q)=\mathrm{D}(p\|p^*)+\mathrm{D}(p^*\|q)+\nabla_{p^*}\mathrm{D}(p^*\|q)^\mathrm{T}(p-p^*),
\end{equation}
for any additional distribution $p^*$, where the constant term $\log e$ in the gradient becomes superfluous, on account of the fact that $\sum_i p_i-p_i^*=0$. Observe that part of the above expression may be readily interpreted as the Taylor expansion of $\mathrm{D}(p\|q)$ about $p^*$,
\begin{equation} \label{eq:Taylor}
\mathrm{D}(p\|q)=\mathrm{D}(p^*\|q)+\nabla_{p^*}\mathrm{D}(p^*\|q)^\mathrm{T}(p-p^*)+O(\|p-p^*\|^2),
\end{equation}
with error precisely $\mathrm{D}(p\|p^*)$.

In the context of convex projections, suppose that we wish to find the closest point $p^*$ inside a convex set $\mathscr{P}$ to a reference point $q$, in KL divergence, succinctly,
\begin{equation*}
p^*=\operatorname*{arg\,min}_{p\in\mathscr{P}} \mathrm{D}(p\|q).
\end{equation*}
This problem is represented in Fig.~\ref{fig:1}. The solution $p^*$ is called the \emph{information projection} of $q$ onto $\mathscr{P}$. Because for such $p^*$ the projection of the gradient of the objective onto the vector difference $p-p^*$ for any $p\in\mathscr{P}$ must be nonnegative, i.e.,
\begin{equation*}
\nabla_{p^*}\mathrm{D}(p^*\|q)^\mathrm{T}(p-p^*)\geqslant 0,
\end{equation*}
we may conclude from the previous equality involving the gradient that
\begin{equation*}
\mathrm{D}(p\|q)\geqslant\mathrm{D}(p\|p^*)+\mathrm{D}(p^*\|q).
\end{equation*}
This last inequality is, in fact, a known generalisation of the Pythagorean theorem for projections onto convex sets, generally involving obtuse triangles\footnote{The expression relating the gradient with a set of divergences shown here may be readily generalise to prove an analogue of the Pythagorean theorem for Bregman projections. Recall that Bregman divergences encompass both squared Euclidean distances and KL divergences as a special case. An alternative proof of the Pythagorean theorem for KL divergences, which inspired a small part of the analysis in this manuscript, can be found in~\cite{cover91b} (Theor.~11.6.1).}.

\begin{figure}[htb]
\centering
\includegraphics[scale=\FigScale]{figures/ConvProj.pdf}
\caption{Information projection $p^*$ of a reference distribution $q$ onto a convex set~$\mathscr{P}$.}
\label{fig:1}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[scale=0.5]{figures/triplex_profiles.pdf}
\caption{Probability simplices showing, the population distribution $q$, the user's profile $p_0$, the updated profile $p_1$.}
\label{fig:2-1}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[scale=0.5]{figures/triplex_alpha.png}
\caption{Probability simplices showing, the population distribution $q=(0.417, 0.333, 0.250)$, the user's profile $p_0=(0.167, 0.333, 0.500)$, the updated profile $p_1=(0.167, 0.167, 0.666)$. The intermediate points show the value of $p_\alpha$ for different $\alpha$.}
\label{fig:2-2}
\end{figure}

\subsection{Differential update of the anonymity risk due to revealing new information}
\label{sec:1-4}

\noindent
Under this simple model, we consider the following problem. Suppose that the distribution $p_0$ represents a history of online activity of a given user up to this time, with associated anonymity risk $\mathrm{D}(p_0\|q)$. Consider now a series of new queries, with interests matching a profile $p_1$ and associated risk $\mathrm{D}(p_1\|q)$ (Fig.~\ref{fig:2-1}). If those new queries were observed, the overall user profile would be updated to
\begin{equation*}
p_\alpha = (1-\alpha)p_0+\alpha p_1,
\end{equation*}
where the activity parameter $\alpha\in(0,1)$ is the fraction of new queries with respect to the total amount of queries released. We investigate the updated anonymity risk (Fig.~\ref{fig:2-2})
\begin{equation*}
\mathrm{D}((1-\alpha)p_0+\alpha p_1\|q), 
\end{equation*}
in terms of the risks associated with the past and current activity, for a marginal activity increment~$\alpha$. To this end, we analyse the first argument of the KL divergence, in the form of a convex combination, through a series of quick preliminary lemmas\footnote{The mathematical proofs and results developed here may be generalised in their entirety from KL divergences to Bregman divergences, and they are loosely inspired by a fundamental Pythagorean inequality for Bregman projections on convex sets.}.

On the one hand, since the KL divergence is a convex function, we may bound the updated risk as
\begin{equation} \label{eq:D:convexity}
\mathrm{D}\left((1-\alpha)p_0+\alpha p_1\middle\|q\right)\leqslant(1-\alpha)\mathrm{D}(p_0\|q)+\alpha\,\mathrm{D}(p_1\|q).
\end{equation}
On the other hand, we may resort to our previous gradient analysis in \S \ref{sec:1-3}, specifically to \eqref{eq:gradient} and \eqref{eq:Taylor}, to write the first\hyph order Taylor approximation
\begin{equation} \label{eq:D:Taylor}
\mathrm{D}\left((1-\alpha)p_0+\alpha p_1\middle\|q\right)=(1-\alpha)\mathrm{D}(p_0\|q)+\alpha\,\mathrm{D}(p_1\|q)-\alpha\,\mathrm{D}(p_1\|p_0)+O(\alpha^2).
\end{equation}

This last expression is consistent with the convexity bound~\eqref{eq:D:convexity}, and quite intuitively, the term $-\alpha\,\mathrm{D}(p_1\|p_0)$ in the Taylor approximation refining the convex bound vanishes for negligible activity $\alpha$ or new activity profile $p_1$ similar to the history $p_0$ revealed thus far. We may alternatively write the updated risk as an increment with respect to that based on the user's online history, as
\begin{equation*}
\mathrm{D}\left((1-\alpha)p_0+\alpha p_1\middle\|q\right)-\mathrm{D}(p_0\|q)=\alpha\left(\mathrm{D}(p_1\|q)-\mathrm{D}(p_0\|q)-\mathrm{D}(p_1\|p_0)\right)+O(\alpha^2),
\end{equation*}
which we observe to be approximately proportional to the relative activity parameter $\alpha$, and to an expression that only depends on the divergences between the profiles involved.

\subsection{Special cases of delta update and uniform reference}
\label{sec:1-5}

\noindent
In the special case when the new activity contains a single query, the new profile $p_1$ is a Kronecker delta $\delta^i$ at some category~$i$. In this case,
\begin{equation*}
\mathrm{D}(p_1\|q)=\mathrm{D}(\delta^i\|q)=-\log q_i,\ \text{and}
\end{equation*}
\begin{equation*}
\mathrm{D}\left((1-\alpha)p_0+\alpha p_1\middle\|q\right)=(1-\alpha)\mathrm{D}(p_0\|q)+\alpha\log \frac{p_{0\,i}}{q_i}+O(\alpha^2).
\end{equation*}
A second corollary follows from taking the reference profile $q$ as the uniform distribution $u=\tfrac{1}{m}$, and replacing KL divergences in \eqref{eq:D:convexity} and \eqref{eq:D:Taylor} with Shannon entropies according to
\begin{equation}\label{eq:DvsH}
\mathrm{D}(p\|u)=\log m-\mathrm{H}(p).
\end{equation}
Precisely,
\begin{equation}\label{eq:H:convexity}
\mathrm{H}\left((1-\alpha)p_0+\alpha p_1\right)\geqslant(1-\alpha)\mathrm{H}(p_0)+\alpha\,\mathrm{H}(p_1).
\end{equation}
consistently with the concavity of the entropy, and
\begin{equation}\label{eq:H:Taylor}
\mathrm{H}\left((1-\alpha)p_0+\alpha p_1\right)=(1-\alpha)\mathrm{H}(p_0)+\alpha\,\mathrm{H}(p_1)+\alpha\,\mathrm{D}(p_1\|p_0)+O(\alpha^2).
\end{equation}
Even more specifically, in the case of a delta update $p_1=\delta^i$ and uniform reference profile,
\begin{equation*}
\mathrm{H}\left((1-\alpha)p_0+\alpha p_1\right)=(1-\alpha)\mathrm{H}(p_0)-\alpha\log p_{0\,i}+O(\alpha^2).
\end{equation*}

\subsection{Best and worst update}
\label{sec:1-6}

\noindent
For a given activity $\alpha$ and history $p_0$, we investigate the profile updates $p_1$ leading to the best and worst overall anonymity risk $\mathrm{D}\left((1-\alpha)p_0+\alpha p_1\middle\|q\right)$. The problem of finding the best profile, yielding the smallest risk, is formally identical to that of optimal query forgery extensively analysed in~\cite{Rebollo10IT}. Note that this problem may also be interpreted as the information projection of the population profile $q$ onto the convex set of possible forged profiles
\begin{equation*}
\mathscr{P}=\left\{(1-\alpha)p_0+\alpha p_1\right\},
\end{equation*}
with fixed $\alpha$ and $p_0$, a scaled, translated probability simplex. In this case, the generalized Pythagorean theorem shown earlier guarantees
\begin{equation*}
\mathrm{D}\left((1-\alpha)p_0+\alpha p_1\middle\|q\right)\geqslant\mathrm{D}\left((1-\alpha)p_0+\alpha p_1^*\middle\|(1-\alpha)p_0+\alpha p_1\right)+\mathrm{D}\left((1-\alpha)p_0+\alpha p_1^*\middle\|q\right).
\end{equation*}

We may now turn to the case of the worst profile update $p_1$, leading to the highest anonymity risk. Consider two distributions $p$ and $q$ on the discrete support alphabet $i=1,\dots,m$, representing predefined categories of interest in our context. Recall that $p$ is said to be \emph{absolutely continuous} with respect to $q$, denoted $p\ll q$, whenever $q_i=0$ implies $p_i=0$ for each~$i$. Otherwise, if for some $i$, we had $p_i>0$ but $q_i=0$, then $\mathrm{D}(p\|q)=\infty$. In the context at hand, we may assume that the population profile incorporates all categories of interest, so that $q_i>0$, which ensures absolute continuity, i.e., $p\ll q$. Therefore, we would like to solve 
\begin{equation*}
\max_{p_1\ll q} \mathrm{D}\left((1-\alpha)p_0+\alpha p_1\middle\|q\right).
\end{equation*}

We shall distinguish two special cases, and leave the general maximisation problem for future investigation. Let us tackle first the simpler case $\alpha=1$, and call $p_1=p$. Recall that the \emph{cross\hyph entropy} between two distributions $p$ and $q$ is defined as
\begin{equation*}
\mathrm{H}(p\|q)=-\sum_{i=1}^m p_i\log q_i,
\end{equation*}
and is related to the (Shannon) entropy and the KL divergence via
\begin{equation*}
\mathrm{H}(p\|q)=\mathrm{H}(p)+\mathrm{D}(p\|q).
\end{equation*}
Clearly,
\begin{equation*}
\max_{p\ll q} \mathrm{H}(p\|q)=-\log q_\text{min},
\end{equation*}
attained for $p=\delta^i$ corresponding to the category $i$ minimising~$q$. It turns out that this is also the solution to the maximisation problem in the divergence, because 
\begin{equation*}
\mathrm{D}(p\|q)=\mathrm{H}(p\|q)-\mathrm{H}(p),
\end{equation*}
and $\mathrm{H}(\delta^i)=0$, which means that $p=\delta^i$ simultaneously maximises the cross\hyph entropy and minimises the entropy.

The second special case we aim to solve is that of a uniform reference $q=u$, discussed in \S \ref{sec:1-5}. The corresponding problem is
\begin{equation*}
\min_{p_1} \mathrm{H}\left((1-\alpha)p_0+\alpha p_1\right).
\end{equation*}
We claim that the worst profile update $p_1$ is again a Kronecker delta, but this time at the category $i$ maximising $p_0$. Indeed, assume without loss of generality that $p_0$ is sorted in decreasing order, observe that $(1-\alpha)p_0+\alpha\delta^1$ majorises any other convex combination $(1-\alpha)p_0+\alpha p_1$, and recall that the entropy is Schur\hyph concave.

As for the general case, the associated cross\hyph entropy problem is fairly simple. We have
\begin{equation}\label{eq:H:worst}
\max_{p_1\ll q} \mathrm{H}\left((1-\alpha)p_0+\alpha p_1\middle\|q\right)=(1-\alpha)\mathrm{H}(p_0\|q)-\alpha\log q_\text{min},
\end{equation}
for $p=\delta^i$ at the category minimising~$q$. Unfortunately, the terms in the difference
\begin{equation*}
\mathrm{D}\left((1-\alpha)p_0+\alpha p_1\middle\|q\right)=\mathrm{H}\left((1-\alpha)p_0+\alpha p_1\middle\|q\right)-\mathrm{H}\left((1-\alpha)p_0+\alpha p_1\right),
\end{equation*}
are respectively maximised and minimised for deltas at different categories, in general, namely that minimising $q$, and that maximising $p_0$. We may however provide an upper bound on the anonymity risk based on these considerations; by virtue of the convexity of the divergence and the previous result on its maximisation,
\begin{equation}\label{eq:D:worst}
\mathrm{D}\left((1-\alpha)p_0+\alpha p_1\middle\|q\right)\leqslant(1-\alpha)\mathrm{D}(p_0\|q)-\alpha\log q_\text{min}.
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental results}

\noindent
In the previous section, we formulated the theoretical problem of the differential update of the anonymity risk of time-varying user profiles due to a marginal release of novel information, based on an information\hyph theoretic measure of anonymity risk, specifically, the Kullback\hyph Leibler (KL) divergence between a user profile and the average population's profile. In this section, we verify the theoretical conclusions drawn in the referred section with a series of numerical examples and experimental scenarios. 

More precisely, we analyse the updated anonymity risk in terms of the profile's history and the current activity, for a given marginal increment $\alpha$. Furthermore, we present how, fixed an activity parameter $\alpha$, and given a certain initial profile, it is possible to identify the best and worst profile update leading to a new privacy risk. All of this is shown for the general case of anonymity risk measured as the KL divergence between a user profile and the overall profile of a population, and for the special case in which the population's profile is assumed uniform, in which divergences become Shannon entropies.
 
The examples simply resort to synthetic values of the reference profiles. As for the experimental scenario, we employ Facebook data. We consider a user sharing some new information through a series of posts on their timeline. We are interested to verify the theoretical analysis carried out in this work. All divergences and entropies are in bits.

\subsection{Synthetic examples}

\noindent
In our first proposed example, we choose an initial profile $p_0 = (1/6, 1/3, 1/2)$, representing a user's past online history, an updated profile $p_1 = (1/6, 1/6, 2/3)$ containing more recent activity, and a population distribution $q=(5/12, 1/3, 1/4)$ of reference, across three hypothetical categories of interest. 
For different values of the recent activity parameter $\alpha$, Fig.~\ref{fig:3a} plots the anonymity risk $\mathrm{D}(p_\alpha\|q)$ of our synthetic example of updated user profile $p_\alpha=(1-\alpha)p_0+\alpha p_1$, with respect to the population's profile $q$, the user's history $p_0$, and the recent activity $p_1$. Specifically, we verify the convexity bound~\eqref{eq:D:convexity} and the first-order Taylor approximation~\eqref{eq:D:Taylor} in our theoretical analysis. In addition, we plot (b) the special case of uniform population profile, in which the anonymity risk becomes $\mathrm{H}(p_\alpha)$. We should hasten to point out that the dually additive relationship~\eqref{eq:DvsH} between KL divergence and entropy translates to vertically reflected versions of analogous plots, verifying the entropic properties \eqref{eq:H:convexity} and~\eqref{eq:H:Taylor}.
\begin{figure*}[htb]
	\centering
	\subfloat[]{\includegraphics[width=0.45\textwidth]{figures/example_1_D.eps}
		\label{fig:3a}}
	\hfil
	\subfloat[]{\includegraphics[width=0.45\textwidth]{figures/example_1_H.eps}
		\label{fig:3b}}
	\caption{For different values of the recent activity parameter $\alpha$, we plot (a) the anonymity risk $\mathrm{D}(p_\alpha\|q)$ of a synthetic example of updated user profile $p_\alpha=(1-\alpha)p_0+\alpha p_1$, with respect to the population's profile $q=(5/12, 1/3, 1/4)$, across three hypothetical categories of interest, where $p_0=(1/6, 1/3, 1/2)$ represents the user's online history, and $p_1=(1/6, 1/6, 2/3)$ contains the recent activity in the form of a histogram. We verify the convexity bound~\eqref{eq:D:convexity} and the first-order Taylor approximation~\eqref{eq:D:Taylor} in our theoretical analysis. In addition, we plot (b) the special case of uniform population profile, in which the anonymity risk becomes $\mathrm{H}(p_\alpha)$.}
	\label{fig:3}
\end{figure*}

In our second example we consider two categories of interest, so that profiles actually represent a binary preference. In this simple setting, profiles are completely determined by a single scalar $p$, corresponding to the relative frequency of one of the two categories, being $1-p$ the other frequency. We fix the activity parameter $\alpha = 1/20$, set the historical profile to $p_0=2/3$, the reference profile to $q=3/5$, and verify the analysis on the worst anonymity risk update of \S\ref{sec:1-6} plotting $D(p_\alpha\|q)$ against profile updates $p_1$ ranging from 0 to~1, where, as usual, $p_\alpha=(1-\alpha)p_0+\alpha p_1$. We illustrate this both for the privacy risk based on the KL divergence, in Fig.~\ref{fig:5a}, and for the special case of Shannon entropy, in Fig.~\ref{fig:5b}. 
\begin{figure*}[htbp]
	\centering
	\subfloat[]{\includegraphics[width=0.45\textwidth]{figures/example_2_D.eps}
		\label{fig:5a}}
	\hfil
	\subfloat[]{\includegraphics[width=0.45\textwidth]{figures/example_2_H.eps}
		\label{fig:5b}}
	\caption{In this example we consider two categories of interest, therefore profiles are completely determined by a single scalar $p$, being $1-p$ the other frequency. We fix the activity parameter $\alpha = 1/20$, set the historical profile to $p_0=2/3$, the reference profile to $q=3/5$, and verify the analysis on the worst anonymity risk update of \S\ref{sec:1-6} plotting $D(p_\alpha\|q)$ against profile updates $p_1$ ranging from 0 to~1. In the entropy case we plot $H(p_\alpha)$.}
	\label{fig:5}
\end{figure*}

In the entropy case, our analysis, summarised in the minimisation problem~\eqref{eq:H:worst}, concluded that the worst update is a delta in the most frequent category. In this simple example with two categories, since $p_0>1/2$, the worst update corresponds to $p_1=1$, giving the lowest entropy. The reference line in the plot corresponds to $\mathrm{H}(p_0)\approx 0.918\,\text{bit}$.
For the more general measure of risk as a divergence, since $q=3/5$, we have $q_\mathrm{min}=2/5$, and the bound~\eqref{eq:D:worst} becomes
$$\mathrm{D}(p_\alpha\|q) \leqslant (1-\alpha) \mathrm{D}(p_0\|q) - \alpha \log_2 q_\mathrm{min} \approx 0.0791,$$
fairly loose for the particular values of this example. The reference line in the plot indicates
$\mathrm{D}(p_0\|q)\approx 0.0137$.

These two examples confirm that new activity certainly has an impact on the overall anonymity risk, in accordance with the quantitative analysis in \S\ref{sec:1-6}. This can of course be regarded from the perspective of introducing dummy queries in order to alter the apparent profile of interests, for example, in line with the problem of optimized query forgering investigated in~\cite{Rebollo10IT}.

\subsection{Experiment based on Facebook data}

\noindent
We continue our verification of the theory presented, this time with experiments based on Facebook data, that is, a realistic scenario for which a population of users is sharing posts on Facebook. For the purpose of this study we have used data extracted from the Facebook-Tracking-Exposed project~\cite{FTE}, where users contribute their data to gain more insights on Facebook personalisation algorithm.

The extracted dataset contained 59\,188 posts of 4\,975 timelines, categorised over 10 categories of interest. We selected two users out of this dataset and considered the total of posts collected for each of them, i.e., their entire timelines. The population distribution for the users in the dataset is expressed by the following PMF: 
$$q=(0.0401,0.0870,0.1485,0.1691,0.1025,0.2081,0.0435,0.0525,0.0558,0.0924).$$
Note that $q$ is computed by taking into account not only the selected users, but the entire population of users across the dataset.

For each user we considered a historical profile comprising of the entirety of their posts minus a window of 15 posts. Over this window we consider a smaller sliding window for computing $p_1$, of $5$ posts, hence we set the activity parameter $\alpha = w/L$, where $L=len(timeline)$ is the total number of posts in the timeline, and $w$ represents the sliding window of 5 posts (Fig.~\ref{fig:6}). For \emph{User A} $\alpha_A = 0.0182$, while for \emph{User B} $\alpha_B= 0.0820$. This choice captures the idea that we want to simulate how the profile changes when the user shares $n$ new posts.

\begin{figure}[htb]
\centering
\includegraphics[scale=0.5]{figures/sliding_window.png}
\caption{The image represents how the user initial profile was computed starting from the timeline data included in the dataset. Furthermore we show how the window $W$ of 15 posts is chosen from the last post of the series and how we considered a sliding window $w$ of 5 posts each time.}
\label{fig:6}
\end{figure}

For User A we consider a series 376 shared posts, and for User B we consider a total of 61 posts. We can express the two users' profiles with the following PMFs:
$$p(A)_0=(0.0146,0.0036,0.0810,0.2311,0.0397,0.1931,0.0156,0.0324,0.3705,0.0179),$$
$$p(B)_0=(0.0159,0.0090,0.0804,0.2280,0.0609,0.1991,0.0194,0.0749,0.2846,0.0274).$$
For the set value of activity parameter $\alpha$, Figs.~\ref{fig:7a},~\ref{fig:7c} plot the anonymity risk $\mathrm{D}(p_\alpha\|q)$ between a user's updated profile $p_\alpha=(1-\alpha)p_0+\alpha p_1$, with respect to the population distribution $q$. Recall that $p_0$ is a user's profile in the Facebook dataset, built taking into consideration a long series of samples. This capture the idea that a user's profile is computed out of their history over a long series of actions. 
\begin{figure*}[htb]
	\centering
	\subfloat[]{\includegraphics[width=0.4\textwidth]{figures/experiment1_A.eps}
		\label{fig:7a}}
	\hfil
	\subfloat[]{\includegraphics[width=0.4\textwidth]{figures/experiment2_A.eps}
		\label{fig:7b}}
	\\
	\subfloat[]{\includegraphics[width=0.4\textwidth]{figures/experiment1_B.eps}
		\label{fig:7c}}
	\hfil
	\subfloat[]{\includegraphics[width=0.4\textwidth]{figures/experiment2_B.eps}
		\label{fig:7d}}
	\caption{The figure considers the privacy risk between a user profile and a reference population distribution for two facebook users (Figs.~\ref{fig:7b},~\ref{fig:7d}), and the risk increment $\Delta\mathcal{R} = \mathrm{D}(p_\alpha\|q) - \mathrm{D}(p_0\|q)$ where $p_0$ is a user's profile in the Facebook dataset and $q$ is the reference population distribution calculated for all the posts in the dataset (Figs.~\ref{fig:7b},~\ref{fig:7d}).}
	\label{fig:7}
\end{figure*}

These experiments confirm the theoretical analysis and examples presented, verifying in a real-world settings the convexity bound~\eqref{eq:D:convexity} and the first-order Taylor approximation~\eqref{eq:D:Taylor} described in our theoretical analysis. In addition, we can computer the bound~\eqref{eq:D:worst} for the general measure of the privacy risk as the KL divergence, which becomes, for User A,
$$\mathrm{D}(p_\alpha\|q) \leqslant (1-\alpha) \mathrm{D}(p_0\|q) - \alpha \log_2 q_\mathrm{min} \approx 0.8870,$$
and for User B,
$$\mathrm{D}(p_\alpha\|q) \leqslant 0.7723.$$
Furthermore, we considered, in Figs. \ref{fig:7b} and~\ref{fig:7d}, the privacy risk increments between the user profiles and an updated profile given by a certain activity over time. Recall that these deltas are computed as 
$$\Delta\mathcal{R} = \mathrm{D}(p_\alpha\|q) - \mathrm{D}(p_0\|q),$$
to show how a certain activity can theoretically result in an anonymity risk gain or loss.

Note that the theoretical analysis and results proposed in this article apply to dynamic profiles that change over time. This aspect is particularly interesting, since we are not simply considering profiles as a snapshot of the user's activity, over a small interval, but we are also taking into account changes in interests and general behaviour that can impact the privacy risk.

As a result we can reach another interesting observation, consisting in the fact that profiles might have different privacy risk in different moments of time. This confirms the intuitive assumption that individuals might change their tastes and interests compared to a reference population, therefore having an impact on their overall privacy risk. In this case we reasonably assume that the profile of certain individuals might change more rapidly over time than that of the entire population.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
\noindent
We proposed a series of refinements of a recent information-theoretic model of a user profile expressed through a histogram of categories of interest. The corresponding privacy risk is measured as the Kullback-Leibler divergence with respect to the histogram accounting for the interests of the overall population. Loosely speaking, this risk may be interpreted as an anonymity risk, in the sense that the interests of a specific user may diverge from those of the general population, extrapolating Jaynes' rationale on maximum-entropy methods.

We investigate the profile updates leading to the best and worst overall anonymity risk for a given activity and history. Thus, we connect the best case to the fairly recent information-theoretic framework of optimised query forgery and tag suppression for privacy protection.

Furthermore, the analysis of our model is applied to an experimental scenario, using Facebook timeline data. Our main objective was measuring how privacy is affected when new content is posted. Often, a user of some online service is unable to verify how much a possible privacy attacker can find out about them. We used real Facebook data to show how our model can be applied to a real world scenario. This aspect is particularly important for content filtering in Facebook. In fact, as users are profiled on Facebook, the very same activity is used to filter the information they are able to access, based on their interests. There is no transparency on Facebook's side about how this filtering and profiling happens. We hope that studies like this might encourage users to seek more transparency in the filtering techniques used by online services in general.

With regard to future work, we would like to express the relationships between users as well as the people they communicate with, taking them all into consideration when calculating users' privacy risk.
